{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../APMM storage/\"\n",
    "name = \"Storage_Volume_Metrics_Hourly_2.csv\"\n",
    "filename = str(path) + str(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_1 = \"Storage_Volume_Metrics_Hourly_1.csv\"\n",
    "filename_1 = str(path) + str(name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename,encoding='utf-16',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv(filename_1,encoding='utf-16',sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data,data1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Hour'] = data['Hour'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "compcounts = data.groupby(['Storage Volume Name'])['Hour'].count().reset_index()\n",
    "compcounts = compcounts[compcounts['Hour'] >= 720]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain only volumes from compcounts\n",
    "data = data[data['Storage Volume Name'].isin(compcounts['Storage Volume Name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data, threshold = 50):\n",
    "    #data preps\n",
    "    #check for duplicate entries\n",
    "    duplicate_rows_df = data[data.duplicated()]\n",
    "    #print(\"number of duplicate rows: \", duplicate_rows_df.shape)\n",
    "    #find missing values\n",
    "    missing_stats = pd.DataFrame(data.isnull().sum()/data.shape[0] * 100, index = None)\n",
    "    missing_stats.reset_index(inplace = True)\n",
    "\n",
    "    #Remove columns with more than 50% nulls\n",
    "    missing_stats.columns = ['Field','Value']\n",
    "    missing_stats['flag'] = missing_stats['Value'].apply(lambda x: 1 if x > threshold else 0)\n",
    "\n",
    "    cols_to_rem = missing_stats['Field'][missing_stats['flag'] == 1]\n",
    "    #print(len(cols_to_rem),\"columns will be removed from analysis with missing values more than 50%\")\n",
    "    #print(cols_to_rem)\n",
    "    data = data.drop(cols_to_rem, axis = 1)\n",
    "    \n",
    "    #remove fields with no variability\n",
    "    #find columns with no variability\n",
    "    var_stats = pd.DataFrame(data.var())\n",
    "    var_stats.reset_index(inplace = True)\n",
    "\n",
    "    var_stats.columns = ['Field','Value']\n",
    "    var_stats['flag'] = var_stats['Value'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "    cols_to_rem = var_stats[var_stats['flag'] == 1]['Field']\n",
    "    data = data.drop(cols_to_rem, axis = 1)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pre_process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cols = [x for x in data.columns if 'Total' not in x]\n",
    "filter_cols = [x for x in filter_cols if 'Maximum' not in x]\n",
    "filter_cols = [x for x in filter_cols if 'Peak' not in x]\n",
    "filter_cols.remove('Overall Transfer Size (KiB/op)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[filter_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create compid for unique components\n",
    "uniqcomponents = data[['Storage System Name','Storage Volume Name']].drop_duplicates()\n",
    "uniqcomponents['compid'] = np.arange(len(uniqcomponents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(uniqcomponents, on = ['Storage System Name','Storage Volume Name'])\n",
    "data = data.drop(['Storage System Name','Storage Volume Name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_cols = ['compid','Hour','Volume Utilization']\n",
    "perf_cols = [x for x in data.columns if x != 'Volume Utilization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hour',\n",
       " 'Overall Read I/O Rate (ops/s)',\n",
       " 'Overall Write I/O Rate (ops/s)',\n",
       " 'Read Data Rate (MiB/s)',\n",
       " 'Write Data Rate (MiB/s)',\n",
       " 'Read Response Time (ms/op)',\n",
       " 'Write Response Time (ms/op)',\n",
       " 'Overall Response Time (ms/op)',\n",
       " 'Read Transfer Size (KiB/op)',\n",
       " 'Write Transfer Size (KiB/op)',\n",
       " 'Write Cache Delay I/O Rate (ops/s)',\n",
       " 'Overall Read Cache Hit Percentage',\n",
       " 'Overall Write Cache Hit Percentage',\n",
       " 'Disk to Cache Transfer Rate (ops/s)',\n",
       " 'Cache to Disk Transfer Rate (ops/s)',\n",
       " 'Write Cache Delay Percentage',\n",
       " 'Read Ahead Percentage of Cache Hits',\n",
       " 'Overall Host Attributed Response Time Percentage',\n",
       " 'Nonpreferred Node Usage Percentage',\n",
       " 'compid']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_df = data[capacity_cols]\n",
    "perf_df = data[perf_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_df = capacity_df.sort_values(by=['compid','Hour'])\n",
    "capacity_df = capacity_df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_df = capacity_df.rename(columns={'Hour':'date','Volume Utilization':'volumeutilization'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findzerocapacity(df):\n",
    "    tmpdf = df.groupby(['compid'])['volumeutilization'].mean().reset_index()\n",
    "    tmpdf = tmpdf[tmpdf['volumeutilization'] > 0]\n",
    "    zerodf = tmpdf[tmpdf['volumeutilization'] <= 0]\n",
    "    df = df[df['compid'].isin(tmpdf['compid'])]\n",
    "    return df,zerodf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_df,zerovols = findzerocapacity(capacity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1594191, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capacity_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1275352"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take 20% of data for testing\n",
    "cutoffindex = int(len(capacity_df) * 0.8)\n",
    "cutoffindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#capacity model starts here\n",
    "#hourly data, take in 12 hours past and predict next 6 hours\n",
    "sequence_in = 12\n",
    "sequence_out = 6\n",
    "\n",
    "def gen_sequence(id_df,seq_in,seq_out,seq_cols):\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    for start, stop in zip(range(0, num_elements-seq_in-seq_out), range(seq_in, num_elements-seq_out)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "def gen_labels(id_df,seq_in,seq_out,label):\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    for start,stop in zip(range(seq_in,num_elements-seq_out),range(seq_in+seq_out,num_elements)):\n",
    "        yield data_matrix[start:stop,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test = [],[]\n",
    "for comps in capacity_df['compid'].unique():\n",
    "    for sequence in gen_sequence(capacity_df[(capacity_df['compid'] == comps) & (capacity_df.index <= cutoffindex)],sequence_in,sequence_out,['volumeutilization']):\n",
    "        X_train.append(sequence)\n",
    "    for sequence in gen_sequence(capacity_df[(capacity_df['compid'] == comps)& (capacity_df.index > cutoffindex)],sequence_in,sequence_out,['volumeutilization']):\n",
    "        X_test.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train,y_test = [],[]\n",
    "\n",
    "for comps in capacity_df['compid'].unique():\n",
    "    for sequence in gen_labels(capacity_df[(capacity_df['compid'] == comps) & (capacity_df.index <= cutoffindex)],sequence_in,sequence_out,['volumeutilization']):\n",
    "        y_train.append(sequence)\n",
    "    for sequence in gen_labels(capacity_df[(capacity_df['compid'] == comps)& (capacity_df.index > cutoffindex)],sequence_in,sequence_out,['volumeutilization']):\n",
    "        y_test.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test = np.asarray(X_train),np.asarray(X_test)\n",
    "y_train,y_test = np.asarray(y_train),np.asarray(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use entity embeddings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_x, train_y,h1=100,h2=50):\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(layers.LSTM(h1, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(layers.Dense(h2, activation='relu'))\n",
    "    model.add(layers.Dense(n_outputs))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleseq = build_model(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3841/3841 [==============================] - 160s 42ms/step - loss: 0.0154 - val_loss: 0.0214\n",
      "Epoch 2/20\n",
      "3841/3841 [==============================] - 140s 36ms/step - loss: 0.0152 - val_loss: 0.0217\n",
      "Epoch 3/20\n",
      "3841/3841 [==============================] - 140s 36ms/step - loss: 0.0151 - val_loss: 0.0212\n",
      "Epoch 4/20\n",
      "3841/3841 [==============================] - 141s 37ms/step - loss: 0.0150 - val_loss: 0.0215\n",
      "Epoch 5/20\n",
      "3841/3841 [==============================] - 193s 50ms/step - loss: 0.0149 - val_loss: 0.0215\n",
      "Epoch 6/20\n",
      "3841/3841 [==============================] - 238s 62ms/step - loss: 0.0148 - val_loss: 0.0210\n",
      "Epoch 7/20\n",
      "3841/3841 [==============================] - 220s 57ms/step - loss: 0.0147 - val_loss: 0.0216\n",
      "Epoch 8/20\n",
      "3841/3841 [==============================] - 372s 97ms/step - loss: 0.0146 - val_loss: 0.0209\n",
      "Epoch 9/20\n",
      "3841/3841 [==============================] - 746s 194ms/step - loss: 0.0145 - val_loss: 0.0214\n",
      "Epoch 10/20\n",
      "3841/3841 [==============================] - 215s 56ms/step - loss: 0.0144 - val_loss: 0.0204\n",
      "Epoch 11/20\n",
      "3841/3841 [==============================] - 221s 57ms/step - loss: 0.0143 - val_loss: 0.0203\n",
      "Epoch 12/20\n",
      "3841/3841 [==============================] - 156s 41ms/step - loss: 0.0143 - val_loss: 0.0205\n",
      "Epoch 13/20\n",
      "3841/3841 [==============================] - 163s 42ms/step - loss: 0.0142 - val_loss: 0.0206\n",
      "Epoch 14/20\n",
      "3841/3841 [==============================] - 168s 44ms/step - loss: 0.0142 - val_loss: 0.0213\n",
      "Epoch 15/20\n",
      "3841/3841 [==============================] - 200s 52ms/step - loss: 0.0141 - val_loss: 0.0215\n",
      "Epoch 16/20\n",
      "3841/3841 [==============================] - 162s 42ms/step - loss: 0.0141 - val_loss: 0.0204\n",
      "Epoch 17/20\n",
      "3841/3841 [==============================] - 146s 38ms/step - loss: 0.0140 - val_loss: 0.0200\n",
      "Epoch 18/20\n",
      "3841/3841 [==============================] - 175s 46ms/step - loss: 0.0140 - val_loss: 0.0199\n",
      "Epoch 19/20\n",
      "3841/3841 [==============================] - 158s 41ms/step - loss: 0.0139 - val_loss: 0.0207\n",
      "Epoch 20/20\n",
      "3841/3841 [==============================] - 205s 53ms/step - loss: 0.0139 - val_loss: 0.0203\n"
     ]
    }
   ],
   "source": [
    "history = simpleseq.fit(X_train,y_train,\n",
    "                   epochs=20,\n",
    "                   batch_size=256,\n",
    "                   validation_split = 0.2,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_drp(train_x, train_y,h1=100,h2=50):\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(layers.LSTM(h1,dropout=0.5, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(layers.Dense(h2, activation='relu'))\n",
    "    model.add(layers.Dense(n_outputs))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = build_model_drp(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h2 = m2.fit(X_train,y_train,\n",
    "#                    epochs=2,\n",
    "#                    batch_size=128,\n",
    "#                    validation_split = 0.2,\n",
    "#                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_drp(train_x, train_y,h1=100,h2=50):\n",
    "    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(layers.LSTM(h1,dropout=0.5,recurrent_dropout=0.3, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "    model.add(layers.Dense(h2, activation='relu'))\n",
    "    model.add(layers.Dense(n_outputs))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = build_model_drp(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h3 = m3.fit(X_train,y_train,\n",
    "#                    epochs=2,\n",
    "#                    batch_size=128,\n",
    "#                    validation_split = 0.2,\n",
    "#                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpleseq.save(str(path)+'apmm_capacity_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpreds = simpleseq.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014092643997681389"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "ytesteval = y_test.reshape(y_test.shape[0],y_test.shape[1]*y_test.shape[2])\n",
    "testmse = mean_squared_error(testpreds,ytesteval)\n",
    "testmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_ts_df = capacity_df.groupby(['compid']).tail(sequence_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut_preds = []\n",
    "\n",
    "for comps in last_ts_df['compid'].unique():\n",
    "    tmpdf = last_ts_df[(last_ts_df['compid'] == comps)]\n",
    "    tmpdf = tmpdf['volumeutilization'].values\n",
    "    tmpdf = tmpdf.reshape(sequence_in,1)\n",
    "    fut_preds.append(tmpdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fut_preds = np.asarray(fut_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "futpreds = simpleseq.predict(fut_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdate = capacity_df.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create future dates\n",
    "futdates = []\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "for i in range(1,sequence_out+1):\n",
    "    ts = maxdate + DateOffset(hours=i)\n",
    "    futdates.append(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = pd.DataFrame()\n",
    "for each in range(len(futpreds)):\n",
    "    xdf = futpreds[each]\n",
    "    xdf[xdf < 0] = 0\n",
    "    xdf[xdf > 1] = 1\n",
    "    xdf = pd.DataFrame(futpreds[each],index=futdates)\n",
    "    xdf['compid'] = each\n",
    "    forecast_df = forecast_df.append(xdf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df.index.name = 'date'\n",
    "forecast_df.columns = ['volumeutilization','compid']\n",
    "forecast_df = forecast_df[['compid','volumeutilization']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_df = forecast_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "capacity_df['flag'] = 'actual'\n",
    "forecast_df['flag'] = 'predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = pd.concat([capacity_df,forecast_df],axis=0)\n",
    "finaldf = finaldf.sort_values(by=['compid','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = finaldf.merge(uniqcomponents , on = 'compid').drop(['compid'],axis=1)\n",
    "finaldf = finaldf[['Storage System Name','Storage Volume Name','date','volumeutilization']]\n",
    "finaldf.to_csv(str(path)+'apmm_capacity.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compid</th>\n",
       "      <th>date</th>\n",
       "      <th>volumeutilization</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-06-25 14:00:00</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-06-25 15:00:00</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-06-25 16:00:00</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-06-25 17:00:00</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-06-25 18:00:00</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>actual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   compid                date  volumeutilization    flag\n",
       "0       0 2020-06-25 14:00:00             0.0003  actual\n",
       "1       0 2020-06-25 15:00:00             0.0003  actual\n",
       "2       0 2020-06-25 16:00:00             0.0003  actual\n",
       "3       0 2020-06-25 17:00:00             0.0003  actual\n",
       "4       0 2020-06-25 18:00:00             0.0003  actual"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apmm",
   "language": "python",
   "name": "apmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
