{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../../../APMM storage/\"\n",
    "name = \"Storage_Volume_Metrics_Hourly_2.csv\"\n",
    "filename = str(path) + str(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_1 = \"Storage_Volume_Metrics_Hourly_1.csv\"\n",
    "filename_1 = str(path) + str(name_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(filename,encoding='utf-16',sep=\"\\t\")\n",
    "data1 = pd.read_csv(filename_1,encoding='utf-16',sep=\"\\t\")\n",
    "data = pd.concat([data,data1],axis=0)\n",
    "\n",
    "del data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Hour'] = data['Hour'].apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "compcounts = data.groupby(['Storage Volume Name'])['Hour'].count().reset_index()\n",
    "compcounts = compcounts[compcounts['Hour'] >= 720]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retain only volumes from compcounts\n",
    "data = data[data['Storage Volume Name'].isin(compcounts['Storage Volume Name'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data, threshold = 50):\n",
    "    #data preps\n",
    "    #check for duplicate entries\n",
    "    duplicate_rows_df = data[data.duplicated()]\n",
    "    #print(\"number of duplicate rows: \", duplicate_rows_df.shape)\n",
    "    #find missing values\n",
    "    missing_stats = pd.DataFrame(data.isnull().sum()/data.shape[0] * 100, index = None)\n",
    "    missing_stats.reset_index(inplace = True)\n",
    "\n",
    "    #Remove columns with more than 50% nulls\n",
    "    missing_stats.columns = ['Field','Value']\n",
    "    missing_stats['flag'] = missing_stats['Value'].apply(lambda x: 1 if x > threshold else 0)\n",
    "\n",
    "    cols_to_rem = missing_stats['Field'][missing_stats['flag'] == 1]\n",
    "    #print(len(cols_to_rem),\"columns will be removed from analysis with missing values more than 50%\")\n",
    "    #print(cols_to_rem)\n",
    "    data = data.drop(cols_to_rem, axis = 1)\n",
    "    \n",
    "    #remove fields with no variability\n",
    "    #find columns with no variability\n",
    "    var_stats = pd.DataFrame(data.var())\n",
    "    var_stats.reset_index(inplace = True)\n",
    "\n",
    "    var_stats.columns = ['Field','Value']\n",
    "    var_stats['flag'] = var_stats['Value'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "    cols_to_rem = var_stats[var_stats['flag'] == 1]['Field']\n",
    "    data = data.drop(cols_to_rem, axis = 1)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pre_process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cols = [x for x in data.columns if 'Total' not in x]\n",
    "filter_cols = [x for x in filter_cols if 'Maximum' not in x]\n",
    "filter_cols = [x for x in filter_cols if 'Peak' not in x]\n",
    "filter_cols.remove('Overall Transfer Size (KiB/op)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[filter_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create compid for unique components\n",
    "uniqcomponents = data[['Storage System Name','Storage Volume Name']].drop_duplicates()\n",
    "uniqcomponents['compid'] = np.arange(len(uniqcomponents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(uniqcomponents, on = ['Storage System Name','Storage Volume Name'])\n",
    "data = data.drop(['Storage System Name','Storage Volume Name'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_cols = [x for x in data.columns if x != 'Volume Utilization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = data[perf_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = perf_df.drop('Overall Response Time (ms/op)',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Read Response Time (ms/op)','Write Response Time (ms/op)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "testindex = perf_df.index.max() - 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = perf_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = perf_df[perf_df.index >= testindex]\n",
    "traindata = perf_df[perf_df.index < testindex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = traindata.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata.to_csv(str(path)+'perf_testdata.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_index('Hour',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add date features as independent variables\n",
    "def adddatefeatures(df):\n",
    "    df.loc[:,'dayofweek'] = df.index.dayofweek\n",
    "    df.loc[:,'month'] = df.index.month\n",
    "    df.loc[:,'hour'] = df.index.hour\n",
    "    return df\n",
    "\n",
    "\n",
    "def addtimedummies(data):\n",
    "    cols = ['month','dayofweek','hour']\n",
    "    for col in cols:\n",
    "        if col == 'month':\n",
    "            max_val = 12\n",
    "            N = max_val -1 #Since month index starts from 1 and dow and hour from 0\n",
    "        elif col == 'dayofweek':\n",
    "            max_val = 6\n",
    "            N = max_val\n",
    "        else:\n",
    "            max_val = 23\n",
    "            N = max_val\n",
    "        df = list()\n",
    "        series = data[col]\n",
    "        for each in series:\n",
    "            vals = list(np.zeros(N+1,dtype='int'))\n",
    "            vals[each-1] = 1\n",
    "            df.append(vals)\n",
    "        names = [str(col)+'_' + str(x) for x in range(1,N+2)]\n",
    "        df = pd.DataFrame(df,columns=names)\n",
    "        df.set_index(data.index,inplace=True)\n",
    "        data = pd.concat([data,df],axis=1)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = adddatefeatures(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Overall Read I/O Rate (ops/s)</th>\n",
       "      <th>Overall Write I/O Rate (ops/s)</th>\n",
       "      <th>Read Data Rate (MiB/s)</th>\n",
       "      <th>Write Data Rate (MiB/s)</th>\n",
       "      <th>Read Response Time (ms/op)</th>\n",
       "      <th>Write Response Time (ms/op)</th>\n",
       "      <th>Read Transfer Size (KiB/op)</th>\n",
       "      <th>Write Transfer Size (KiB/op)</th>\n",
       "      <th>Write Cache Delay I/O Rate (ops/s)</th>\n",
       "      <th>Overall Read Cache Hit Percentage</th>\n",
       "      <th>...</th>\n",
       "      <th>Disk to Cache Transfer Rate (ops/s)</th>\n",
       "      <th>Cache to Disk Transfer Rate (ops/s)</th>\n",
       "      <th>Write Cache Delay Percentage</th>\n",
       "      <th>Read Ahead Percentage of Cache Hits</th>\n",
       "      <th>Overall Host Attributed Response Time Percentage</th>\n",
       "      <th>Nonpreferred Node Usage Percentage</th>\n",
       "      <th>compid</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hour</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-06-25 14:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.043056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.327563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.372037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>12.869167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-25 15:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.028889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.338553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.407127</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275556</td>\n",
       "      <td>12.790278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-25 16:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.026271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.412056</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.348023</td>\n",
       "      <td>12.383333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-25 17:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.031667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.402531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.231111</td>\n",
       "      <td>12.689444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-25 18:00:00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.038611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.323616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.409735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>13.172778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Overall Read I/O Rate (ops/s)  \\\n",
       "Hour                                                 \n",
       "2020-06-25 14:00:00                            0.0   \n",
       "2020-06-25 15:00:00                            0.0   \n",
       "2020-06-25 16:00:00                            0.0   \n",
       "2020-06-25 17:00:00                            0.0   \n",
       "2020-06-25 18:00:00                            0.0   \n",
       "\n",
       "                     Overall Write I/O Rate (ops/s)  Read Data Rate (MiB/s)  \\\n",
       "Hour                                                                          \n",
       "2020-06-25 14:00:00                        1.043056                     0.0   \n",
       "2020-06-25 15:00:00                        1.028889                     0.0   \n",
       "2020-06-25 16:00:00                        1.026271                     0.0   \n",
       "2020-06-25 17:00:00                        1.031667                     0.0   \n",
       "2020-06-25 18:00:00                        1.038611                     0.0   \n",
       "\n",
       "                     Write Data Rate (MiB/s)  Read Response Time (ms/op)  \\\n",
       "Hour                                                                       \n",
       "2020-06-25 14:00:00                 0.004453                         0.0   \n",
       "2020-06-25 15:00:00                 0.004428                         0.0   \n",
       "2020-06-25 16:00:00                 0.004422                         0.0   \n",
       "2020-06-25 17:00:00                 0.004435                         0.0   \n",
       "2020-06-25 18:00:00                 0.004473                         0.0   \n",
       "\n",
       "                     Write Response Time (ms/op)  Read Transfer Size (KiB/op)  \\\n",
       "Hour                                                                            \n",
       "2020-06-25 14:00:00                     0.327563                          0.0   \n",
       "2020-06-25 15:00:00                     0.338553                          0.0   \n",
       "2020-06-25 16:00:00                     0.341866                          0.0   \n",
       "2020-06-25 17:00:00                     0.308831                          0.0   \n",
       "2020-06-25 18:00:00                     0.323616                          0.0   \n",
       "\n",
       "                     Write Transfer Size (KiB/op)  \\\n",
       "Hour                                                \n",
       "2020-06-25 14:00:00                      4.372037   \n",
       "2020-06-25 15:00:00                      4.407127   \n",
       "2020-06-25 16:00:00                      4.412056   \n",
       "2020-06-25 17:00:00                      4.402531   \n",
       "2020-06-25 18:00:00                      4.409735   \n",
       "\n",
       "                     Write Cache Delay I/O Rate (ops/s)  \\\n",
       "Hour                                                      \n",
       "2020-06-25 14:00:00                                 0.0   \n",
       "2020-06-25 15:00:00                                 0.0   \n",
       "2020-06-25 16:00:00                                 0.0   \n",
       "2020-06-25 17:00:00                                 0.0   \n",
       "2020-06-25 18:00:00                                 0.0   \n",
       "\n",
       "                     Overall Read Cache Hit Percentage  ...  \\\n",
       "Hour                                                    ...   \n",
       "2020-06-25 14:00:00                              0.000  ...   \n",
       "2020-06-25 15:00:00                              0.000  ...   \n",
       "2020-06-25 16:00:00                              0.000  ...   \n",
       "2020-06-25 17:00:00                              0.000  ...   \n",
       "2020-06-25 18:00:00                              0.125  ...   \n",
       "\n",
       "                     Disk to Cache Transfer Rate (ops/s)  \\\n",
       "Hour                                                       \n",
       "2020-06-25 14:00:00                             0.333333   \n",
       "2020-06-25 15:00:00                             0.275556   \n",
       "2020-06-25 16:00:00                             0.348023   \n",
       "2020-06-25 17:00:00                             0.231111   \n",
       "2020-06-25 18:00:00                             0.186667   \n",
       "\n",
       "                     Cache to Disk Transfer Rate (ops/s)  \\\n",
       "Hour                                                       \n",
       "2020-06-25 14:00:00                            12.869167   \n",
       "2020-06-25 15:00:00                            12.790278   \n",
       "2020-06-25 16:00:00                            12.383333   \n",
       "2020-06-25 17:00:00                            12.689444   \n",
       "2020-06-25 18:00:00                            13.172778   \n",
       "\n",
       "                     Write Cache Delay Percentage  \\\n",
       "Hour                                                \n",
       "2020-06-25 14:00:00                           0.0   \n",
       "2020-06-25 15:00:00                           0.0   \n",
       "2020-06-25 16:00:00                           0.0   \n",
       "2020-06-25 17:00:00                           0.0   \n",
       "2020-06-25 18:00:00                           0.0   \n",
       "\n",
       "                     Read Ahead Percentage of Cache Hits  \\\n",
       "Hour                                                       \n",
       "2020-06-25 14:00:00                                  0.0   \n",
       "2020-06-25 15:00:00                                  0.0   \n",
       "2020-06-25 16:00:00                                  0.0   \n",
       "2020-06-25 17:00:00                                  0.0   \n",
       "2020-06-25 18:00:00                                  0.0   \n",
       "\n",
       "                     Overall Host Attributed Response Time Percentage  \\\n",
       "Hour                                                                    \n",
       "2020-06-25 14:00:00                                          0.000437   \n",
       "2020-06-25 15:00:00                                          0.000425   \n",
       "2020-06-25 16:00:00                                          0.000419   \n",
       "2020-06-25 17:00:00                                          0.000466   \n",
       "2020-06-25 18:00:00                                          0.000447   \n",
       "\n",
       "                     Nonpreferred Node Usage Percentage  compid  dayofweek  \\\n",
       "Hour                                                                         \n",
       "2020-06-25 14:00:00                                 0.0       0          3   \n",
       "2020-06-25 15:00:00                                 0.0       0          3   \n",
       "2020-06-25 16:00:00                                 0.0       0          3   \n",
       "2020-06-25 17:00:00                                 0.0       0          3   \n",
       "2020-06-25 18:00:00                                 0.0       0          3   \n",
       "\n",
       "                     month  hour  \n",
       "Hour                              \n",
       "2020-06-25 14:00:00      6    14  \n",
       "2020-06-25 15:00:00      6    15  \n",
       "2020-06-25 16:00:00      6    16  \n",
       "2020-06-25 17:00:00      6    17  \n",
       "2020-06-25 18:00:00      6    18  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = addtimedummies(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['hour','dayofweek','month'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function between features and targets\n",
    "train_features = [x for x in data.columns if x not in targets]\n",
    "train_features = [x for x in data.columns if x not in 'compid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssx = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5,shuffle=True,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssx = MinMaxScaler()\n",
    "ssy = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfmodel(X,y):\n",
    "    inputs = layers.Input(shape=(X.shape[1],))\n",
    "    x = layers.Dense(256, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(64,activation='relu')(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    y = layers.Dense(y.shape[1],activation='linear')(x)\n",
    "    out10 = layers.Dense(y.shape[1])(x)\n",
    "    out50 = layers.Dense(y.shape[1])(x)\n",
    "    out90 = layers.Dense(y.shape[1])(x)\n",
    "    model = Model(inputs=inputs,outputs=[out10,out50,out90])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_loss(q,y,f):\n",
    "    e = (y-f)\n",
    "    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, save_model,load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [lambda y,f: q_loss(0.1,y,f), lambda y,f: q_loss(0.5,y,f), lambda y,f: q_loss(0.9,y,f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### TRAINING ON BATCH 1 #####\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x1c767f8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to identify source code of lambda function <function <lambda> at 0x1c767f8b0>. It was defined in this code:\n",
      "losses = [lambda y,f: q_loss(0.1,y,f), lambda y,f: q_loss(0.5,y,f), lambda y,f: q_loss(0.9,y,f)]\n",
      "\n",
      "This code must contain a single distinguishable lambda. To avoid this problem, define each lambda in a separate expression.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x1c767f8b0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to identify source code of lambda function <function <lambda> at 0x1c767f8b0>. It was defined in this code:\n",
      "losses = [lambda y,f: q_loss(0.1,y,f), lambda y,f: q_loss(0.5,y,f), lambda y,f: q_loss(0.9,y,f)]\n",
      "\n",
      "This code must contain a single distinguishable lambda. To avoid this problem, define each lambda in a separate expression.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x1c767fca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to identify source code of lambda function <function <lambda> at 0x1c767fca0>. It was defined in this code:\n",
      "losses = [lambda y,f: q_loss(0.1,y,f), lambda y,f: q_loss(0.5,y,f), lambda y,f: q_loss(0.9,y,f)]\n",
      "\n",
      "This code must contain a single distinguishable lambda. To avoid this problem, define each lambda in a separate expression.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x1c767fca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to identify source code of lambda function <function <lambda> at 0x1c767fca0>. It was defined in this code:\n",
      "losses = [lambda y,f: q_loss(0.1,y,f), lambda y,f: q_loss(0.5,y,f), lambda y,f: q_loss(0.9,y,f)]\n",
      "\n",
      "This code must contain a single distinguishable lambda. To avoid this problem, define each lambda in a separate expression.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x1c767f940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to identify source code of lambda function <function <lambda> at 0x1c767f940>. It was defined in this code:\n",
      "losses = [lambda y,f: q_loss(0.1,y,f), lambda y,f: q_loss(0.5,y,f), lambda y,f: q_loss(0.9,y,f)]\n",
      "\n",
      "This code must contain a single distinguishable lambda. To avoid this problem, define each lambda in a separate expression.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x1c767f940> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Unable to identify source code of lambda function <function <lambda> at 0x1c767f940>. It was defined in this code:\n",
      "losses = [lambda y,f: q_loss(0.1,y,f), lambda y,f: q_loss(0.5,y,f), lambda y,f: q_loss(0.9,y,f)]\n",
      "\n",
      "This code must contain a single distinguishable lambda. To avoid this problem, define each lambda in a separate expression.\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "5041/5041 [==============================] - 13s 3ms/step - loss: 0.0159 - dense_81_loss: 0.0083 - dense_82_loss: 0.0241 - dense_83_loss: 0.0206\n",
      "Epoch 2/2\n",
      "5041/5041 [==============================] - 12s 2ms/step - loss: 0.0065 - dense_81_loss: 0.0041 - dense_82_loss: 0.0112 - dense_83_loss: 0.0062\n",
      "##### TRAINING ON BATCH 2 #####\n",
      "Epoch 1/2\n",
      "5041/5041 [==============================] - 15s 3ms/step - loss: 0.0159 - dense_87_loss: 0.0081 - dense_88_loss: 0.0244 - dense_89_loss: 0.0206\n",
      "Epoch 2/2\n",
      "5041/5041 [==============================] - 14s 3ms/step - loss: 0.0067 - dense_87_loss: 0.0042 - dense_88_loss: 0.0115 - dense_89_loss: 0.0065\n",
      "##### TRAINING ON BATCH 3 #####\n",
      "Epoch 1/2\n",
      "5041/5041 [==============================] - 14s 3ms/step - loss: 0.0163 - dense_93_loss: 0.0082 - dense_94_loss: 0.0248 - dense_95_loss: 0.0213\n",
      "Epoch 2/2\n",
      "5041/5041 [==============================] - 14s 3ms/step - loss: 0.0066 - dense_93_loss: 0.0042 - dense_94_loss: 0.0113 - dense_95_loss: 0.0063\n",
      "##### TRAINING ON BATCH 4 #####\n",
      "Epoch 1/2\n",
      "5041/5041 [==============================] - 14s 3ms/step - loss: 0.0158 - dense_99_loss: 0.0082 - dense_100_loss: 0.0240 - dense_101_loss: 0.0206\n",
      "Epoch 2/2\n",
      "5041/5041 [==============================] - 14s 3ms/step - loss: 0.0066 - dense_99_loss: 0.0042 - dense_100_loss: 0.0114 - dense_101_loss: 0.0064\n",
      "##### TRAINING ON BATCH 5 #####\n",
      "Epoch 1/2\n",
      "5041/5041 [==============================] - 15s 3ms/step - loss: 0.0156 - dense_105_loss: 0.0079 - dense_106_loss: 0.0240 - dense_107_loss: 0.0201\n",
      "Epoch 2/2\n",
      "5041/5041 [==============================] - 14s 3ms/step - loss: 0.0064 - dense_105_loss: 0.0041 - dense_106_loss: 0.0111 - dense_107_loss: 0.0062\n"
     ]
    }
   ],
   "source": [
    "overall_mse = []\n",
    "overall_mae = []\n",
    "minmse = np.Inf\n",
    "folds = 1\n",
    "\n",
    "for train_index , test_index in kf.split(data):\n",
    "    print(f\"##### TRAINING ON BATCH {folds} #####\")\n",
    "    x_train,x_val = data.loc[train_index,train_features],data.loc[test_index,train_features]\n",
    "    y_train,y_val = data.loc[train_index,targets],data.loc[test_index,targets]\n",
    "    \n",
    "    #scale x and y data\n",
    "    x_train = ssx.fit_transform(x_train)\n",
    "    y_train = ssy.fit_transform(y_train)\n",
    "    \n",
    "    #fit model\n",
    "    perf_model = perfmodel(x_train,y_train)\n",
    "    perf_model.compile(loss=losses,optimizer='adam',loss_weights=[0.3,0.3,0.3])\n",
    "    perf_model.fit(x_train,[y_train,y_train,y_train],epochs=2,batch_size=256,verbose=1)\n",
    "    \n",
    "    #transform val set and make preds\n",
    "    x_val = ssx.transform(x_val)\n",
    "    yhat = perf_model.predict(x_val)[1]\n",
    "    y_val = ssy.transform(y_val)\n",
    "    \n",
    "    #compare preds to y_val\n",
    "    perf_mse = mean_squared_error(y_val,yhat)\n",
    "    perf_mae = mean_absolute_error(y_val,yhat)\n",
    "    overall_mse.append(perf_mse)\n",
    "    overall_mae.append(perf_mae)\n",
    "    \n",
    "    #get least mse and save that model\n",
    "    if perf_mse <= minmse:\n",
    "        minmse = perf_mse\n",
    "        perf_model.save('perf_model.h5')\n",
    "    \n",
    "    folds += 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "perfmods = load_model('perf_model.h5',custom_objects={'losses':losses},compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions on full data\n",
    "full_preds = perfmods.predict(ssx.transform(data.loc[:,train_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create df for pred outputs\n",
    "def quantile_outputs(data,preds,targets,ssy):\n",
    "    pred_df = pd.DataFrame()\n",
    "    for i in range(len(preds)):\n",
    "        df = pd.DataFrame(ssy.inverse_transform(preds[i]))\n",
    "        if i == 0:\n",
    "            dfcols = [x + '_lb' for x in targets]\n",
    "        elif i == 1:\n",
    "            dfcols = [x + '_pred' for x in targets]\n",
    "        else:\n",
    "            dfcols = [x + '_ub' for x in targets]\n",
    "        df.columns = dfcols\n",
    "        pred_df = pd.concat([pred_df,df],axis=1)\n",
    "    pred_df['compid'] = data['compid']\n",
    "    return pred_df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = quantile_outputs(data,full_preds,targets,ssy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_cols = [x for x in pred_df.columns if x not in 'compid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df_mod = pred_df[trans_cols].values\n",
    "pred_df_mod[pred_df_mod < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.loc[:,trans_cols] = pred_df_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = traindata.merge(pred_df,left_index=True,right_index=True,on='compid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = traindata.merge(uniqcomponents,on=['compid']).drop(['compid'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.to_csv(str(path)+'apmm_perf.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apmm",
   "language": "python",
   "name": "apmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
